//! Lexer for NGQL using `logos`.
//!
//! Tokenizes NGQL queries into a stream of tokens.

use logos::Logos;

/// NGQL Token types.
///
/// Generated by `logos` for high-performance lexing.
#[derive(Logos, Debug, Clone, PartialEq)]
#[logos(skip r"[ \t\n\r]+")] // Skip whitespace
pub enum Token<'a> {
    // =========================================================================
    // Keywords (case-insensitive)
    // =========================================================================
    /// MATCH keyword
    #[token("MATCH", ignore(ascii_case))]
    Match,

    /// OPTIONAL keyword
    #[token("OPTIONAL", ignore(ascii_case))]
    Optional,

    /// MERGE keyword
    #[token("MERGE", ignore(ascii_case))]
    Merge,

    /// WHERE keyword
    #[token("WHERE", ignore(ascii_case))]
    Where,

    /// CASE keyword
    #[token("CASE", ignore(ascii_case))]
    Case,

    /// WHEN keyword
    #[token("WHEN", ignore(ascii_case))]
    When,

    /// THEN keyword
    #[token("THEN", ignore(ascii_case))]
    Then,

    /// ELSE keyword
    #[token("ELSE", ignore(ascii_case))]
    Else,

    /// END keyword
    #[token("END", ignore(ascii_case))]
    End,

    /// WITH keyword
    #[token("WITH", ignore(ascii_case))]
    With,

    /// UNWIND keyword
    #[token("UNWIND", ignore(ascii_case))]
    Unwind,

    /// RETURN keyword
    #[token("RETURN", ignore(ascii_case))]
    Return,

    /// AND keyword
    #[token("AND", ignore(ascii_case))]
    And,

    /// OR keyword
    #[token("OR", ignore(ascii_case))]
    Or,

    /// NOT keyword
    #[token("NOT", ignore(ascii_case))]
    Not,

    /// AS keyword (for aliasing)
    #[token("AS", ignore(ascii_case))]
    As,

    /// IN keyword
    #[token("IN", ignore(ascii_case))]
    In,

    /// TRUE literal
    #[token("TRUE", ignore(ascii_case))]
    True,

    /// FALSE literal
    #[token("FALSE", ignore(ascii_case))]
    False,

    /// NULL literal
    #[token("NULL", ignore(ascii_case))]
    Null,

    /// COALESCE function
    #[token("COALESCE", ignore(ascii_case))]
    Coalesce,

    /// toLower function
    #[token("toLower", ignore(ascii_case))]
    ToLower,

    /// toUpper function
    #[token("toUpper", ignore(ascii_case))]
    ToUpper,

    /// contains function
    #[token("contains", ignore(ascii_case))]
    Contains,

    /// startsWith function
    #[token("startsWith", ignore(ascii_case))]
    StartsWith,

    /// endsWith function
    #[token("endsWith", ignore(ascii_case))]
    EndsWith,

    /// split function
    #[token("split", ignore(ascii_case))]
    Split,

    /// toString function
    #[token("toString", ignore(ascii_case))]
    ToString,

    /// toInteger function
    #[token("toInteger", ignore(ascii_case))]
    ToInteger,

    /// toFloat function
    #[token("toFloat", ignore(ascii_case))]
    ToFloat,

    /// toBoolean function
    #[token("toBoolean", ignore(ascii_case))]
    ToBoolean,

    /// date function
    #[token("date", ignore(ascii_case))]
    DateFunc,

    /// datetime function
    #[token("datetime", ignore(ascii_case))]
    DateTimeFunc,

    /// id() function
    #[token("id", ignore(case))]
    IdFunc,

    /// type() function
    #[token("type", ignore(case))]
    TypeFunc,

    /// COUNT aggregation function
    #[token("count", ignore(case))]
    Count,

    /// SUM aggregation function
    #[token("SUM", ignore(ascii_case))]
    Sum,

    /// AVG aggregation function
    #[token("AVG", ignore(ascii_case))]
    Avg,

    /// MIN aggregation function
    #[token("MIN", ignore(ascii_case))]
    Min,

    /// MAX aggregation function
    #[token("MAX", ignore(ascii_case))]
    Max,

    /// COLLECT aggregation function
    #[token("COLLECT", ignore(ascii_case))]
    Collect,

    /// DISTINCT keyword
    #[token("DISTINCT", ignore(ascii_case))]
    Distinct,

    /// ORDER keyword
    #[token("ORDER", ignore(ascii_case))]
    Order,

    /// BY keyword
    #[token("BY", ignore(ascii_case))]
    By,

    /// GROUP keyword
    #[token("GROUP", ignore(ascii_case))]
    Group,

    /// ASC keyword (ascending sort)
    #[token("ASC", ignore(ascii_case))]
    Asc,

    /// DESC keyword (descending sort)
    #[token("DESC", ignore(ascii_case))]
    Desc,

    /// LIMIT keyword
    #[token("LIMIT", ignore(ascii_case))]
    Limit,

    /// VECTOR_SIMILARITY function (Sprint 13)
    #[token("vector_similarity", ignore(ascii_case))]
    VectorSimilarity,

    /// CLUSTER function (Sprint 16) - community detection
    #[token("cluster", ignore(ascii_case))]
    Cluster,

    // =========================================================================
    // Debugging / Profiling (Sprint 29)
    // =========================================================================
    /// EXPLAIN keyword - show query plan
    #[token("EXPLAIN", ignore(ascii_case))]
    Explain,

    /// PROFILE keyword - execute and show timing
    #[token("PROFILE", ignore(ascii_case))]
    Profile,

    // =========================================================================
    // Transaction Keywords (Sprint 50)
    // =========================================================================
    /// BEGIN keyword
    #[token("BEGIN", ignore(ascii_case))]
    Begin,

    /// COMMIT keyword
    #[token("COMMIT", ignore(ascii_case))]
    Commit,

    /// ROLLBACK keyword
    #[token("ROLLBACK", ignore(ascii_case))]
    Rollback,

    // =========================================================================
    // Time-Travel Keywords (Sprint 54)
    // =========================================================================
    /// AT keyword - for temporal queries (AT TIME '2026-01-15T12:00:00Z')
    #[token("AT", ignore(ascii_case))]
    At,

    /// TIME keyword - used with AT for temporal queries
    #[token("TIME", ignore(ascii_case))]
    Time,

    /// TIMESTAMP keyword - alternative to TIME
    #[token("TIMESTAMP", ignore(ascii_case))]
    Timestamp,

    /// FLASHBACK keyword - revert database state
    #[token("FLASHBACK", ignore(ascii_case))]
    Flashback,

    /// TO keyword - used with FLASHBACK TO
    #[token("TO", ignore(ascii_case))]
    To,

    // =========================================================================
    // Mutation Keywords (Sprint 21+)
    // =========================================================================
    /// CREATE keyword - create nodes/edges
    #[token("CREATE", ignore(ascii_case))]
    Create,

    /// DELETE keyword - delete nodes/edges
    #[token("DELETE", ignore(ascii_case))]
    Delete,

    /// SET keyword - update properties
    #[token("SET", ignore(ascii_case))]
    Set,

    /// DETACH keyword - used with DELETE to remove edges first
    #[token("DETACH", ignore(ascii_case))]
    Detach,

    /// Star `*` for COUNT(*)
    #[token("*")]
    Star,

    // =========================================================================
    // Symbols
    // =========================================================================
    /// Left parenthesis `(`
    #[token("(")]
    LParen,

    /// Right parenthesis `)`
    #[token(")")]
    RParen,

    /// Left bracket `[`
    #[token("[")]
    LBracket,

    /// Right bracket `]`
    #[token("]")]
    RBracket,

    /// Left brace `{`
    #[token("{")]
    LBrace,

    /// Right brace `}`
    #[token("}")]
    RBrace,

    /// Right arrow `->`
    #[token("->")]
    RightArrow,

    /// Left arrow `<-`
    #[token("<-")]
    LeftArrow,

    /// Dash `-`
    #[token("-")]
    Dash,

    /// Colon `:`
    #[token(":")]
    Colon,

    /// Comma `,`
    #[token(",")]
    Comma,

    /// Dot `.`
    #[token(".")]
    Dot,

    /// Double Dot `..`
    #[token("..")]
    DoubleDot,

    /// Equals `=`
    #[token("=")]
    Eq,

    /// Not equals `<>`
    #[token("<>")]
    Neq,

    /// Less than `<`
    #[token("<")]
    Lt,

    /// Greater than `>`
    #[token(">")]
    Gt,

    /// Less than or equal `<=`
    #[token("<=")]
    Lte,

    /// Greater than or equal `>=`
    #[token(">=")]
    Gte,

    // =========================================================================
    // Literals
    // =========================================================================
    /// String literal (double-quoted)
    #[regex(r#""([^"\\]|\\.)*""#, |lex| lex.slice())]
    String(&'a str),

    /// String literal (single-quoted)
    #[regex(r#"'([^'\\]|\\.)*'"#, |lex| lex.slice())]
    StringSingle(&'a str),

    /// Integer literal
    #[regex(r"-?[0-9]+", |lex| lex.slice())]
    Integer(&'a str),

    /// Float literal
    #[regex(r"-?[0-9]+\.[0-9]+([eE][-+]?[0-9]+)?", priority = 2, callback = |lex| lex.slice())]
    Float(&'a str),

    // =========================================================================
    // Identifiers
    // =========================================================================
    /// Identifier (variable name, label, etc.)
    #[regex(r"[a-zA-Z_][a-zA-Z0-9_]*", |lex| lex.slice())]
    Ident(&'a str),

    /// Parameter ($name)
    #[regex(r"\$[a-zA-Z_][a-zA-Z0-9_]*", |lex| lex.slice())]
    Param(&'a str),
}

impl<'a> Token<'a> {
    /// Returns true if this token is a keyword.
    pub fn is_keyword(&self) -> bool {
        matches!(
            self,
            Token::Match
                | Token::Optional
                | Token::Merge
                | Token::Where
                | Token::Case
                | Token::When
                | Token::Then
                | Token::Else
                | Token::End
                | Token::With
                | Token::Unwind
                | Token::Return
                | Token::And
                | Token::Or
                | Token::Not
                | Token::As
                | Token::In
                | Token::True
                | Token::False
                | Token::Null
                | Token::Count
                | Token::Sum
                | Token::Avg
                | Token::Min
                | Token::Max
                | Token::Collect
                | Token::Distinct
                | Token::Order
                | Token::By
                | Token::Group
                | Token::Asc
                | Token::Desc
                | Token::Limit
                | Token::VectorSimilarity
                | Token::Cluster
                | Token::Explain
                | Token::Profile
                | Token::Begin
                | Token::Commit
                | Token::Rollback
                | Token::IdFunc
                | Token::TypeFunc
                | Token::At
                | Token::Time
                | Token::Timestamp
                | Token::Flashback
                | Token::To
        )
    }

    /// Returns true if this token is a comparison operator.
    pub fn is_comparison(&self) -> bool {
        matches!(
            self,
            Token::Eq | Token::Neq | Token::Lt | Token::Gt | Token::Lte | Token::Gte
        )
    }

    /// Returns true if this token is an aggregation function.
    pub fn is_aggregate(&self) -> bool {
        matches!(
            self,
            Token::Count | Token::Sum | Token::Avg | Token::Min | Token::Max | Token::Collect
        )
    }
}

/// Tokenize an NGQL query string into a vector of tokens.
///
/// # Example
///
/// ```
/// use neural_parser::lexer::tokenize;
///
/// let tokens = tokenize("MATCH (n) RETURN n").unwrap();
/// assert_eq!(tokens.len(), 6);
/// ```
pub fn tokenize(input: &str) -> Result<Vec<Token<'_>>, LexError> {
    let mut lexer = Token::lexer(input);
    let mut tokens = Vec::new();

    while let Some(result) = lexer.next() {
        match result {
            Ok(token) => tokens.push(token),
            Err(_) => {
                return Err(LexError {
                    position: lexer.span().start,
                    slice: lexer.slice().to_string(),
                });
            }
        }
    }

    Ok(tokens)
}

/// Error during lexing.
#[derive(Debug, Clone)]
pub struct LexError {
    /// Position in the input where the error occurred.
    pub position: usize,
    /// The problematic slice of input.
    pub slice: String,
}

impl std::fmt::Display for LexError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "Unexpected token '{}' at position {}",
            self.slice, self.position
        )
    }
}

impl std::error::Error for LexError {}

// =============================================================================
// Tests
// =============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_keywords() {
        let tokens = tokenize("MATCH WHERE RETURN AND OR NOT EXPLAIN PROFILE").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Match,
                Token::Where,
                Token::Return,
                Token::And,
                Token::Or,
                Token::Not,
                Token::Explain,
                Token::Profile,
            ]
        );
    }

    #[test]
    fn test_keywords_case_insensitive() {
        let tokens = tokenize("match Match MATCH").unwrap();
        assert_eq!(tokens, vec![Token::Match, Token::Match, Token::Match]);
    }

    #[test]
    fn test_symbols() {
        let tokens = tokenize("() [] -> <- - : , . ..").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::LParen,
                Token::RParen,
                Token::LBracket,
                Token::RBracket,
                Token::RightArrow,
                Token::LeftArrow,
                Token::Dash,
                Token::Colon,
                Token::Comma,
                Token::Dot,
                Token::DoubleDot,
            ]
        );
    }

    #[test]
    fn test_comparison_operators() {
        let tokens = tokenize("= <> < > <= >=").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Eq,
                Token::Neq,
                Token::Lt,
                Token::Gt,
                Token::Lte,
                Token::Gte,
            ]
        );
    }

    #[test]
    fn test_string_literals() {
        let tokens = tokenize(r#""hello" 'world'"#).unwrap();
        assert_eq!(
            tokens,
            vec![Token::String("\"hello\""), Token::StringSingle("'world'")]
        );
    }

    #[test]
    fn test_number_literals() {
        let tokens = tokenize("42 -10 3.14 -2.5").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Integer("42"),
                Token::Integer("-10"),
                Token::Float("3.14"),
                Token::Float("-2.5"),
            ]
        );
    }

    #[test]
    fn test_identifiers() {
        let tokens = tokenize("n person Person_Name _private").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Ident("n"),
                Token::Ident("person"),
                Token::Ident("Person_Name"),
                Token::Ident("_private"),
            ]
        );
    }

    #[test]
    fn test_parameters() {
        let tokens = tokenize("$name $query_param").unwrap();
        assert_eq!(
            tokens,
            vec![Token::Param("$name"), Token::Param("$query_param")]
        );
    }

    #[test]
    fn test_simple_query() {
        let tokens = tokenize("MATCH (n) RETURN n").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Match,
                Token::LParen,
                Token::Ident("n"),
                Token::RParen,
                Token::Return,
                Token::Ident("n"),
            ]
        );
    }

    #[test]
    fn test_pattern_query() {
        let tokens = tokenize("MATCH (a)-[:KNOWS]->(b)").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Match,
                Token::LParen,
                Token::Ident("a"),
                Token::RParen,
                Token::Dash,
                Token::LBracket,
                Token::Colon,
                Token::Ident("KNOWS"),
                Token::RBracket,
                Token::RightArrow,
                Token::LParen,
                Token::Ident("b"),
                Token::RParen,
            ]
        );
    }

    #[test]
    fn test_where_clause() {
        let tokens = tokenize("WHERE n.age > 30").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Where,
                Token::Ident("n"),
                Token::Dot,
                Token::Ident("age"),
                Token::Gt,
                Token::Integer("30"),
            ]
        );
    }

    #[test]
    fn test_order_by_limit_tokens() {
        let tokens = tokenize("ORDER BY n.age DESC LIMIT 10").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Order,
                Token::By,
                Token::Ident("n"),
                Token::Dot,
                Token::Ident("age"),
                Token::Desc,
                Token::Limit,
                Token::Integer("10"),
            ]
        );
    }

    #[test]
    fn test_group_by_tokens() {
        let tokens = tokenize("GROUP BY n.label").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Group,
                Token::By,
                Token::Ident("n"),
                Token::Dot,
                Token::Ident("label"),
            ]
        );
    }

    #[test]
    fn test_time_travel_tokens() {
        let tokens = tokenize("AT TIME '2026-01-15T12:00:00Z'").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::At,
                Token::Time,
                Token::StringSingle("'2026-01-15T12:00:00Z'"),
            ]
        );

        let tokens = tokenize("AT TIMESTAMP '2026-01-15'").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::At,
                Token::Timestamp,
                Token::StringSingle("'2026-01-15'"),
            ]
        );

        let tokens = tokenize("FLASHBACK TO '2026-01-14T00:00:00Z'").unwrap();
        assert_eq!(
            tokens,
            vec![
                Token::Flashback,
                Token::To,
                Token::StringSingle("'2026-01-14T00:00:00Z'"),
            ]
        );
    }
}
